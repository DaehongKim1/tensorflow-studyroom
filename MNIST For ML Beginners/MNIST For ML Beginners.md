이번 튜토리얼은 머신러닝을 해보지 않고 당연히 Tensorflow가 처음인 분들을 위한 튜토리얼입니다. 만약에 이글을 읽는데 MNIST가 무엇인지 알고 있고 softmax (multinomial logistic) regression이 무언인지 안다면 faster paced tutotrial을 보시는것을 권장합니다.

일반적으로 프로그래밍을 처음 배울때 가장 먼저 해보는 것이 "Hello World" 를 출력하는 일입니다. 머신러닝을 처음 시작할때는 MNIST입니다.

MNIST는 손글씨로 작성된 숫자 0에서 9가 적힌 이미지 6만장이 들어 있습니다.
![as](https://github.com/yuby/tensorflow-studyroom/blob/master/MNIST%20For%20ML%20Beginners/images/10.JPG)
![MNIST](images/2.jpg)

그리고 이 데이터 셋의 이미지들은 각각 라벨(0~9)을 가지고 있는데 이게 우리에게 어떤 숫자인지를 알려주고 있습니다. 예를들면 위의 이미지는 5,0,4,1의 라벨을 가지는 이미지입니다.

이 튜토리얼에서는 우리는 이미지를 보고 어떤 숫자인지를 예측하는 모델을 훈련시킬예정입니다. 우리의 목표는 비록 실제로 정교한 모델을 훈련시키는 것은 아니지만 - 물론 나중에 할것입니다만 -  TensorFlow를 사용해서 머신러닝에 대한 감을 잡는 일에 초점을 맞추겠습니다.

이를테면 Softmax Regression 이라고 불리는 간단한 모델을 가지고 시작을 해보겠습니다.

실제로 이 튜토리얼의 코드는 매우 짧습니다. 오직 3줄의 코드만으로 엄청나게 흥미로운 일들이 벌어지게 될것입니다. 하지만 우리에게는 이 일이 일어나게 되는 실제 뒤에서 일어나는 일들을 이해하는 것이 더욱중요합니다. 예를들면 Tensorflow가 어떻게 동작을 하고 있으면 머신러닝의 핵심개념이 무엇인지 입니다. 이때문에 우리는 매우 세심히 코드를 하나하나 분석해 나갈것입니다.

시작해 보겠습니다.

우선 MNIST데이터가 필요하겠죠? 데이터는 Yann LeCun's wesite로 가셔서 받으셔야 합니다. 아니면 조금의 편의를 위해서 파이썬코드를 사용해서 해당데이터를 불러올수 있도록 해보겠습니다. 직접다운 받으시던지 아니면 그냥 아래코드를 사용하시던지 편하신 방법을 선택하세요.

```
import input_data
mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)
```

우선 데이터를 보면 크게 트레이닝데이터(mnist.train) 60,000 세트와 와 10,000개의 테스트 데이터(mnist.test) 세트로 구성되어있습니다. 이 구분이 매우 중요한데 이유는 머신러닝에 있어서 6만개의 데이터를 통해 학습한 내용을 1만개의 아직 배우지 않은 데이터를 통해 모델을 일반화하기 위해서 반드시 필요한 부분이기 때문입니다.

앞서 이야기 했듯이 MNIST의 데이터는 두개의 구성으로 이뤄져잇습니다. 손글씨로 쓰여진 숫자이미지와 이에 상응하는 라벨. 이 둘을 앞으로 이미지는  xs그리고 라벨은 ys로 부르겠습니다.

트레이닝 데이터와 테스트데이터 모두 xs와 ys를 가지고 있습니다. 예를들면 트레이닝 이미지는 mnist.train.images 그리고 라벨의 경우에는 mnist.train.labels입니다.

각각의 이미지는 28x28 픽셀로 구성되어있는데 이를 우리는 배열로 해석할수가 있습니다.

![MNIST](./image/3.jpg)

좌측의 이미지를 배열로 바라보면 우측과 같습니다. 우측의 각 컴퍼넌트는 0과 1사이의 값으로 명암의 정도를 숫자로 나타내고 있습니다.


28행 28열 배열을 784(28x28) 차원의 벡터로 단순화 할 수가 있습니다. 즉 2차원의 데이터형태를 1차원형태의 선형적인(배열) 형태로 데이터를 만들어버립니다. 어떻게 배열을 단순화 시키는지는 중요하지 않습니다. 다만 이미지와의 일관성만 유지가 되면 됩니다.
결국 우리는 MNIST의 이미지를 784차원의 벡터 공간상의 점들의 모음으로 볼수가 있습니다.

784차원의 공간속의 모든벡터가 MNIST 숫자는 아닙니다. 이 공간속에서 보통의 점들은 매우 다릅니다. 보통의 점들이 어떤 형태인지에 대한 개념을 가지기 위해서 우리는 몇몇 점들을 골라서 확인 할수있습니다. 28x28 이미지의 임의의 점의 픽셀은 검은색, 흰색 또는 그림자인 회색 중의 하나입니다.
결국 그 임의의 점들은 마치 아래의 이미지 처럼 노이즈 같이 보입니다.

![MNIST](./image/4.jpg)

실제 MNIST숫자의 이미지는 매우 드문 형태의 이미지 입니다. MNIST의 데이터 점들은 784차원의 공간속에 모두 들어 가있지만 실제로 그 데이터는 매우 이부분의 공간속에서만 존재하고 있기 때문입니다. 즉 전체 공간속에 의미있는 데이터를 담고있는 점들은 매우 일부분이기 때문입니다.


그런데 이렇게 데이터를 단순화 하면서 2차원 구조의 이미지 정보를 버리게 됩니다. 이게 나쁜걸까요?? 글쎄요.. 일단 우리는 이렇게 진행해 보겠습니다.

mnist.train.images의 결과로 우리는 tensor(n 차원 배열)을 [60000, 784] 형태로 얻게 됩니다. 첫번째 값은 60000 개의 트레이닝 이미지 데이터상에서 몇번째 이미지 인가를, 두번째 값의 경우에는 각 이미지의 픽셀의 인덱스정보를 담습니다. 특정 이미지의 특정 픽셀의 값은 0과 1사이의 숫자를통해 그 색의 강도를 나타냅니다.

![MNIST](./image/5.jpg)

그리고 이에 상응하는 MNIST에서의 라벨정보는 0에서 9사이의 숫자로 이뤄져있습니다. 그리고 이 라벨을 "one-hot vectors"의 형태로 사용하는 데 이는 0이 대부분의 차원을 그리고 1은 하나의 차원을 의미합니다. 여기에서 n번째 숫자는 n번째 차원에 1의 값을 가집니다.
예를들면 0은 [1,0,0,0,0,0,0,0,0,0,0] 을 나타냅니다.
mnist.train.labels는 [60000,10]의 형태를 가집니다.
아래는 5,0,4,1을 나타낸 이미지 입니다.

![MNIST](./image/6.jpg)



와.. 뭐가 엄청 복잡해보이는데 그냥 이거 입니다. 각 이미지를 픽셀로 보니까 어라? 이거 28 by 28 배열같은데?? 일단 오케이!

음 그리고 이거 해당 픽셀의 색의 진하기를 0에서 1사이로 표현을 하자! 그럼 어라?? 이차원 배열에 이미지 형태로 특정 0에서 1사이 값들을 가지네??

아! 그리고 아까 보니까 이미지 라벨을 사용해서 해당 이미지가 어떤 값인지도 나타내야겠다!

위에 한참을 떠든 이렇고 저렇고의 내용은 이게 다입니다.

이렇게 만들어진 데이터를 가지고 이제부터 우리의 모델을 만들어 보겠습니다.



이제 우리는 각 이미지를 보고 이미지가 나타내는 숫자가 무엇인지에 대한 확률을 원합니다. 즉 1이라는 그림의 이미지를 보고 우리가 만들 모델이 이 그림은 '1'일 확률이 얼마이다 라는 정보가 필요합니다.
예를 들어서 9가 적힌 이미지를 보고 80%의 확률로 9일것이라고 예측을 하지만 10%의 확률로는 8일것이라고 예측을 하고 있습니다. 그리고 나머지 숫자들이 남은 확률을 나눠가지고 있을것입니다. 아마 8과 9는 둘다 윗부분이 'ㅇ' 모양이라 두드러진 확률을 가진듯합니다.

이런식으로 하나의 대상을 두고 여러다른 것들중에 하나일 확률을 정하는 작업을 softmax가 하는 역활입니다. 지금 같이 이런 간단한 모델을 구성하는데도 softmax regression은 매우 자연스럽고 간단하게 처리가 가능합니다. 그리고 나중에 복잡한 모델을 만드는 경우라도 결국에 마지막 단계에서는 softmax의 단계가 필요하게 됩니다.

이과정을 다이그램으로 나타내면 아래와 같습니다.

![MNIST](http://eric-yuan.me/wp-content/uploads/2014/03/4.png)


Input에 6만장의 숫자 이미지를 입력합니다. 그리고 Feature1에서 각 이미지의 세부적인 특징들로 1차 분류를 하고 Feature2에서는  Feature1의 특징으로 부터 다시한번 특징을 분류해 냅니다.  즉 입력된 이미지가 0~9로 분류가 이뤄졌습니다.

이제 저 노란색 부분에 위의 이미지 5,0,4,1 네개의 이미지(4개의 클래스)를 줍니다. 그럼 결과로
</br>P(y=5 | x) -> 입력 x가 y=5일 확률
</br>P(y=0 | x) -> 입력 x가 y=0일 확률
</br>P(y=4 | x) -> 입력 x가 y=4일 확률
</br>P(y=1 | x) -> 입력 x가 y=1일 확률

를 얻습니다. 즉 5가 적힌 이미지를 보고 이게 5인가라는 질문을 하면 {0.99 , 0 , 0 , 0} 라는 답을 얻게 됩니다. 즉 너의 질문에는 99%정도가 5일듯하구나 정도의 답을 내어두고 이를 5라고 판단하는것은 결국 사람의 몫입니다.


즉 softmax regression은 각 입력의 특징을 모아서 특정 클래스로 구분을 지어두고 이를 바탕으로 확률을 얻는 과정입니다.



일단 지금까지의 내용을 간단하게 정리해보면 이렇습니다. 뭔가 그럴싸 해보이는 데 요점은 이거 입니다.
엄청많은 개랑 고양이 사진 두개를 하나하나 이건 개야. 그리고 이건 고양이야.. 라고 그냥 계속.. 그냥.. 계속.. 데이터가 있는 만큼.. 계속 컴퓨터에게 가르치는 작업을 하면서 개와 고양이의 특징을 하나로 합칩니다.

그리고는 개 사진을 보여주고 이게 뭐냐고 물었을때 개일확률은 90% 고양이일 확률은 10%입니다라는 결론을 내어놓게 만드는것입니다.

원래는 이런 인식기술이 발전하는데는 불과 몇년전까지만 해도 너무나 먼미래의 일이라고 했었는데 요즘은 정말 엄청난 데이터들이 매일매일 생성이 되고 있어서 이 데이터를 가지고 미친듯이 컴퓨터에게 가르치는 작업이 우리가 아는 빅데이터와 머신러닝입니다.


자그럼 다시 본론으로 돌아와 6만개의 MNIST이미지를 분석했더니 아래와 같은 결과를 가지 게 되었습니다.

![MNIST](./image/8.jpg)

여기서 빨간색과 파란색의 두가지 정보가 눈에 띕니다. 우선 0이라는 이미지를 죄다 분석을 해봤더니 각 픽셀의 짙기의 정도가 짙었던 부분은 양수값으로 그리고 색이 상대적으러 옅었던 부분은 음수값으로 가중치를 가지게 됩니다.
그리고 이 가중치와 0 클래스(즉 0이라는 라벨값을 가진 이미지)의 각 픽셀을 곱합니다. 그리고 이 작업을 MNIST의 라벨이  0인 이미지의 갯수만큼 반복을 합니다. 즉, 0이라는 글자를 학습을 하는 것입니다. 그렇게 각 픽셀들의 합을 모두 더하면 각 픽셀의 음, 양수 값이 나오게 됩니다. 그래서 양수값이 나온부분은 파란색으로 음수값이 나온 부분은 빨간색으로 표현을 하면 위와 같은 이미지가 나오게 됩니다.

이제 이러한 과정을 식으로 나타내면 아래와 같습니다.

![MNIST](./image/9.jpg)

클래스에 해당하는 모든 이미지의 픽셀의 값과 가중치를 곱한 총합과 bias를 더해 evidence를 구합니다. i는 클래스를 j는 픽셀의 인덱스를 나타냅니다. 그리고 그렇게 구해진 evidence를 softmax 함수의 인자로 전달을 하면 확률값 y가 나오게 됩니다.

![MNIST](./image/10.jpg)



우리는 softmax regression을 도식화하는 것이 가능합니다. 비록 더많은 입력이 표현되야 하지만 아래와 같이 표현가능합니다.

![MNIST](./image/13.jpg)

각각의 결과는 입력에 대한 가중치를 곱한 총 합에 bias를 더한값을 softmax에 적용하고 있습니다. 위의 그림을 식으로 나타내면 다음과 같습니다.

![MNIST](./image/14.jpg)

이를 벡터화 시켜서 배열의 곱과 벡터의 합으로 나타낼수있습니다. 이러한 방식으로 생각하는것이 계산을 하는데 훨씬 직관적입니다.

![MNIST](./image/15.jpg)

조금더 극단적으로 단순화 시키면 다음과 같습니다.


이제 코드로 표현을 할 시간입니다.

파이썬의 Numpy같은 라이브러리를 사용하면 비용이 많이 드는 작업들 예를들면 파이썬외부의 다른언어로 구현된 배열의 곱과 같은 작업을 처리하는데 매우 효과적입니다. 하지만 불행히도 여전히 모든 연산을 파이썬으로 가져와 작업을 하는데는 오버해드가 있습니다. 이 오버해드는 특히 GPU에서 연산을 처리하기를 원하거나 데이터 전송에 비용이 많이드는 분산처리를 하는 경우에 더 않좋습니다.

TensorFlow 역시 이런 무거운 작업을 수행할때 위와같은 오버해드를 피하기 위한 방법이 으로 연산이 과중한  파이썬으로 부터 독립적인 작업을 수행하는 대신에 Tensflow는 작업간의 상호작용을 그래프 형태로 표현하게 해서 모든 연산을 파이썬 외부에서 동작하게 합니다.

우선 파이썬에서 TensorFlow를 사용하기 위해서는TensorFlow를 import 해야합니다.
import tensorflow as tf

이제 파이썬 내부에서는 tf라는 변수를 통해서 파이썬과 TensorFlow간의 상호작용이 일어나게 됩니다.

```
x = tf.placeholder("float", [None, 784])
```

x 는 특별한 변수값이 아닙니다. 이는 placeholder 를 표현한것으로 우리가 TensorFlow에게 연산을 요청할때 입력하게 되는 값을 의미합니다. 우리는 MNIST 이미지의 어떠한 숫자값도 784 차원의 벡터값으로 문제가 없기를 바랍니다. 그래서 이를 소수형태의 2차원 tensor형태인 [None, 784] 형태로 표현을 합니다.
여기서 None의 의미는 어떠한 길이의 차원을 허용하겠다는 의미입니다.

이제 우리는 가중치(weight)외 biase를 구해야합니다. 우리는 이 값들을 추가적인 입력값으로 생각하고 구현을 해도 되지만 TensorFlow는 이를 Variable 이라는 형태로 훨씬 효율적으로 값을 다룰수 있게 해줍니다.

Variable는 수정이 가능한 텐서입니다.
>
※ Tensor(텐서) : 텐서는 벡터의 확장된 개념입니다.
그러니까 예를들어 우리는 행렬이라는 개념에 대해서 알고 있습니다.
<br/>a(11) b(12) c(13)
<br/>d(21) e(22) f(23)
<br/>g(31) h(32) i(33)
<br/>의 형태의 행렬은 우선 첨자가 두개이고 이를 2차원 평면에 표현을 할수가 있습니다. n*n 행렬의 경우에는 n제곱 만큼의 원소의 갯수가 있습니다. 위의 경우에는 n=3입니다.
그리고 여기서 첨자란 Aij 의 i,j를 의미합니다. 즉 2개의 첨자를 가지므로 rank 2 텐서라고 표현을 할수 있습니다.
이제 대충 텐서의 개념은 Tij (0< i,j < n)으로 구성된 일차원적인 형태의 표현이고 이는 n * n 형태 (텐서를 차원의 개념으로 보면됨) 로 다시 표현해 낼수 있습니다.
즉, 사람이 표현하기 힘든 차원의 형태를 단순히 첨자의 형태로 나타낸 개념으로 이해를 하면 됩니다.


Variable은 수정이 가능한 텐서로 TensorFlow의 작업상호작용 그래프내에 존재합니다. 이는 연산에 의해 사용되기도 하고 수정이 되기도 합니다. 머신러닝 어플리케이션에서는 Variable가 모델의 매개변수가 됩니다.

```
W = tf.Variable(tf.zeros([784,10]))
b = tf.Variable(tf.zeros([10]))
```

tf.Variable을 통해서 Variable의 최초 값을 생성합니다. 이경우에는 W와 b 에 0으로 가득찬 최초의 텐서를 가지게 됩니다.  우리는 이제 W와 b의 값을 채워나갈것이기 때문에 최초의 값이 무엇인지는 의미가 없습니다.
W는 [784,10] 형태를 가집니다. 왜냐하면 784 차원의이미지 벡터를 각기다른 클래스의 벡터 evidence를 곱해서 10차원의 벡터로 만들기 위해서입니다.
b는 [10]의 형태를 가지는데 우리는 최종 출력이전에 이 값을 더할것입니다.

이제 우리는 이 모든 값을을 모델로 만드는 일을 합니다. 딱 한줄이면 됩니다.

```
y = tf.nn.softmax(tf.matmul(x,W) + b)
```

tf.matmul(x,W)를 통해서 두개의 곱합니다. 그리고 b를 곱하고난 결과를 tf.nn.softmax를 적용해 줍니다. 이게 끝입니다. 그냥 간단한 세팅 몇줄과 마지막 이 한줄이면 우리의 모델이 정의 가 됩니다. 하지만 이게 단지
TensorFlow가 softmax regression을 특별히 쉽게 구현하기 위해서 이렇게 된것이 아니라, 수많은 연산방법들을 구현하는데 매우 유연하게 만들어 졌기 때문입니다. 그리고 일단 한번 모델이 정의 가 되면 우리는 이를 다른 디바이스에서 동작할수 있습니다. CPU나 GPU 심지어 폰에서도 동작이 가능합니다.


이렇게 만들어진 모델을 훈련시키기 위해서는 우리는 우리의 모델이 좋은 모델인지에 대한 정의가 필요합니다. 실제로 머신러닝에서 우리는 일반적으로 모델이 나쁘다는 말은 비용과 손실의 개념을 가지고 말을 합니다. 그리고 이를 줄이는것에 집중을 합니다. 하지만 비용과 손실을 결국 같은 개념입니다.

가장 일반적이고 가장 좋은 비용함수를 우리는 "cross entropy" 라고 부릅니다. 놀랍게도 cross entropy 는 정보의 압축코드에서 나온 아이디어 입니다. 하지만 지금은 머신러닝은 물론 겜블링등 에서도 사용이 될만큼 다양한 분야에서 사용이 되고 있습니다. 이 함수는 아래와 같이 정의가 됩니다.

![MNIST](./image/17.jpg)

여기서 y는 우리의 예상 확률분포를 의미하고 y`의 경우에는 실제 확률분포를 의미합니다. 대략적으로  cross entropy는 얼마나 우리의 예측이 효과적으로 사실을 어떻게 측정하는지에 대한 개념입니다.  cross entropy에 대한 더 많은 정보는 이 튜토리얼의 범위를 넘어서기 때문에 자세한 내용을 이곳에서 확인 하시면 됩니다.

 cross entropy를 적용하기 위해서는 우리는 새로운  placeholder를 추가해 제대로된 값을 얻기위한 인풋을 설정하는 일부터 입니다.

```
y_ = tf.placeholder("float", [None,10])
```

그리고 나서  cross entropy를 적용합니다.

cross_entropy = -tf.reduce_sum(y_*tf.log(y))
이는 단지 하나의 예측에 대한 cross entropy가 아니라 우리가 보고있는 100개의 이미지의 cross entropy의 합입니다. 100개의 데이터 포인트들이 하나의 데이터 포인트 보다 모델이 훨씬 좋은지를 표현하는데 의미가 있습니다.

이제 우리는 우리의 모델을 가지고 무엇을 해야하는지 알고 있습니다. TensorFlow를 가지고 모델을 훈련시키는 것은 매우 쉽습니다. 왜냐하면 TensorFlow는 이미 여러분의 전체 연산 그래프를 모두 알고 있기 때문입니다. 이는 자동으로 backpropagation algorithm을 사용해서 cost를 낮추기 위해서 효과적으로 변수들을 정의하게 됩니다. 그리고나서 우리가 선택한 최적화 알고리즘을 통해 변수들을 수정해서 cost를 더욱 감소시키게 됩니다.

```
train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)
```

우리는 여기서 cross entropy를 최소화 시키기 위해서 gradient descent 알고리즘을 leaning rate 를 0.01로 해서 적용했습니다.  gradient descent는 간단한 프로시져로 TensorFlow에서 변수들의 살짝식만 방향내에서 이동시키는 역활을해서 cost를 감소 시킵니다. TensorFlow는 이뿐만아니라 다양한 최적화 알고리즘을 제공합니다.


그럼 여기서 정확하게 TensorFlow가 하는 뒤에서 하는 일이 무엇일까요. TensorFlow는 backpropagation 과 gradient descent 가 적용된 그래프에 새로운 작업들을 추가 합니다. 그리고는 gradient descent 트레이닝을 통해서 변수들을 조금씩 변경하는 작업을 통해 비용을 줄이게 됩니다.

그리고 마지막으로 우리의 트레이닝 모델을 실행하기 전에 마지막으로 우리가 만든 모델을 초기화 시키는 작업을 추가해야합니다.

```
init = tf.initialize_all_variables()
```

이제 우리는 우리가 만든 모델을 Session위에서 변수의 초기화를 시작으로 작업을 시작합니다.

```
sess = tf.Session()
sess.run(init)
```

그리고는 학습을 시작합니다. 1000번 반복해서 학습을 진행합니다.

```
for i in range(1000):
  batch_xs, batch_ys = mnist.train.next_batch(100)
  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})
```

매번 루프가 실행이 될때마다 우린 batch 라는 100개의 임의의 데이터 포인트를 우리의 트레이닝 셋으로 부터 얻게 됩니다. 그리고 이 데이터를 train_step을 실행하는데 placeholder를 대체하는 값으로 들어가게 됩니다.

이런 작은 임의로 얻은 작은 batches를 우리는 stochastic 트레이닝이라고 부릅니다. 이경우에는 stochastic gradient descent가 됩니다. 이상적으로는 우리는 우리의 모든데이터를 각 트레이닝의 단계마다 사용하고 싶어합니다. 하지만 이는 매우 비용이 많이 드는 작업입니다. 대신 우리는 각 트레이닝의 단계마다 매번 다른 데이터 셋을 사용하여 더 싸고 동일한 효율을 얻을수 있는 방법을 사용합니다.


그럼 우리가 이렇게 만든 모델이 잘 구현이 된 상태일까요?

우선 우리는 이미지와 label 간의 예측이 제대로 이뤄졌는지 확인을 해야 합니다. tf.argmax는 tensor 상의 특정 축에서 가장 높은entry의  index 값을 리턴하는 매우 유용한 함수입니다. 예를들어 tf.argmax(_y,1)는 정확한 라벨값을 가진 경우이고 tf.argmax(y,1)는 입력에 대한 모델이 생각하는 라벨의 결과입니다. 이 둘을 tf.equal 함수를 사용해서 얼마나 일치하는지를 확인합니다.

```
correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))
```

이는 우리에게 booleans 값의 리스트를 전달합니다. 그리고 이를 소수값으로 변경합니다. 예를들면 [True, False, True, False] 를 [1,0,1,0] 으로 변환하여이를 0.5로 전달하는데 이말은 정확도가 50%라는 말과 동일합니다.

```
accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))
```

마지막으로 우리의 테스트 데이터에 대한 정확도르르 묻는 코드입니다.

```
print sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})
```

결과로 91%를 출력합니다.

91%.. 만족하시나요? 이정도는 나쁜축에 듭니다. 왜냐하면 지금 우리는 매우 명확한 형태의 입력데이터로 매우 간단한 형태로 모델을 만들고 있는데 91%는 만족하지 못할결과입니다. 약간의 변경을 준다면 97%까지 끌어 올릴수 있습니다. 최적의 모델을 사용한다면 99.7%라는 엄청난 정확도를 만들어 낼수도 있습니다.  더 많은 정보를 원하시면 이곳에서 확인 하시면 됩니다.

이제 초보자 수준으로 MNIST를 분석하는 방법을 배웠으니 다음은 전문가 수준으로 한번 해보겠습니다. 일단 위의 내용을 제대로 이해하셔서 다음 챕터에서 MNIST의 전문가가 될 준비를 해주세요!!



http://www.tensorflow.org/tutorials/mnist/beginners/index.md





